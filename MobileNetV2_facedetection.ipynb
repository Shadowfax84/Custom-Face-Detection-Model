{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check all Images are of a valid format</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def check_images(directory):\n",
    "    bad_images = []\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()\n",
    "            except Exception as e:\n",
    "                print(f\"Invalid image: {file_path} - {e}\")\n",
    "                bad_images.append(file_path)\n",
    "    return bad_images\n",
    "\n",
    "# Specify your LFW dataset path\n",
    "lfw_path = r\"C:\\College_programs\\MVT_CNN\\lfw_subset\"\n",
    "bad_images = check_images(lfw_path)\n",
    "\n",
    "if bad_images:\n",
    "    print(\"Found invalid images:\")\n",
    "    for img in bad_images:\n",
    "        print(img)\n",
    "else:\n",
    "    print(\"All images are valid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Delete Invalid Images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def delete_invalid_images(image_dir, log_file=\"deleted_images.log\"):\n",
    "    with open(log_file, \"w\") as log:\n",
    "        for subdir, _, files in os.walk(image_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                try:\n",
    "                    img = tf.io.read_file(file_path)\n",
    "                    img = tf.io.decode_jpeg(img)  # Adjust based on your image format\n",
    "                except Exception as e:\n",
    "                    print(f\"Deleting invalid image: {file_path} ({e})\")\n",
    "                    os.remove(file_path)\n",
    "                    log.write(f\"{file_path}\\n\")  # Log deleted image path\n",
    "\n",
    "# Usage\n",
    "delete_invalid_images(r\"C:\\College_programs\\MVT_CNN\\lfw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a LFW Subset of 100 random classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 18 classes to C:\\College_programs\\MVT_CNN\\lfw_subset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define path to your original LFW dataset\n",
    "lfw_path = r\"C:\\College_programs\\MVT_CNN\\lfw\"  # Replace with your actual path\n",
    "# Define path for your new subset dataset\n",
    "subset_path = r\"C:\\College_programs\\MVT_CNN\\lfw_subset_5\"  # Path for new subset\n",
    "\n",
    "# Step 1: Select 100 random unique classes\n",
    "all_classes = os.listdir(lfw_path)  # List all class directories (individuals)\n",
    "random_classes = random.sample(all_classes, 500)  # Randomly select 100 classes\n",
    "\n",
    "# Step 2: Create new directory structure and move images\n",
    "if not os.path.exists(subset_path):\n",
    "    os.makedirs(subset_path)\n",
    "\n",
    "for class_name in random_classes:\n",
    "    class_dir = os.path.join(lfw_path, class_name)\n",
    "    if os.path.isdir(class_dir):  # Check if it's a directory\n",
    "        # Create a corresponding directory in the subset path\n",
    "        new_class_dir = os.path.join(subset_path, class_name)\n",
    "        os.makedirs(new_class_dir, exist_ok=True)\n",
    "        \n",
    "        # Move all images from original class directory to new class directory\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            src_file = os.path.join(class_dir, img_file)\n",
    "            shutil.move(src_file, new_class_dir)  # Use move instead of copy\n",
    "\n",
    "print(f\"Moved {len(random_classes)} classes to {subset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocess the LFW Subset, rescale, train test split, load generators and get the names of the classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 895 images belonging to 500 classes.\n",
      "Found 224 images belonging to 500 classes.\n",
      "Class indices saved to C:\\College_programs\\MVT_CNN\\lfw_subset_class_indices.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define path to your LFW dataset\n",
    "lfw_path = r\"C:\\College_programs\\MVT_CNN\\lfw_subset\"  # Replace with your actual path\n",
    "\n",
    "# Create ImageDataGenerator for LFW\n",
    "lfw_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.4)\n",
    "\n",
    "# Load training data\n",
    "lfw_train_generator = lfw_datagen.flow_from_directory(\n",
    "    lfw_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data\n",
    "lfw_validation_generator = lfw_datagen.flow_from_directory(\n",
    "    lfw_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Get number of classes from the training generator\n",
    "num_classes = len(lfw_train_generator.class_indices)\n",
    "\n",
    "# Save class indices and names to a text file\n",
    "class_indices = lfw_train_generator.class_indices\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = r\"C:\\College_programs\\MVT_CNN\\lfw_subset_class_indices.txt\"  # Replace with your desired output path\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for class_name, class_index in class_indices.items():\n",
    "        f.write(f\"{class_index}: {class_name}\\n\")\n",
    "\n",
    "print(f\"Class indices saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create a QMUL-SurvFace Subset of 100 random classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 13 classes to C:\\College_programs\\MVT_CNN\\qmul_subset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define path to your original QMUL-SurvFace dataset\n",
    "qmul_path = r\"C:\\College_programs\\MVT_CNN\\QMUL-SurvFace\\Training_Set\"  # Replace with your actual path\n",
    "# Define path for your new subset dataset\n",
    "subset_path = r\"C:\\College_programs\\MVT_CNN\\qmul_subset\"  # Path for new subset\n",
    "\n",
    "# Step 1: Select 100 random unique classes (directories)\n",
    "all_classes = os.listdir(qmul_path)  # List all class directories (identities)\n",
    "random_classes = random.sample(all_classes, 100)  # Randomly select 100 classes\n",
    "\n",
    "# Step 2: Create new directory structure and move images\n",
    "if not os.path.exists(subset_path):\n",
    "    os.makedirs(subset_path)\n",
    "\n",
    "for class_name in random_classes:\n",
    "    class_dir = os.path.join(qmul_path, class_name)\n",
    "    if os.path.isdir(class_dir):  # Check if it's a directory\n",
    "        # Create a corresponding directory in the subset path\n",
    "        new_class_dir = os.path.join(subset_path, class_name)\n",
    "        os.makedirs(new_class_dir, exist_ok=True)\n",
    "        \n",
    "        # Move all images from original class directory to new class directory\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            src_file = os.path.join(class_dir, img_file)\n",
    "            shutil.move(src_file, new_class_dir)  # Use move instead of copy\n",
    "\n",
    "print(f\"Moved {len(random_classes)} classes to {subset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preprocess the QMUL-SurvFace Subset, rescale, train test split, load generators and get the names of the classes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14835 images belonging to 500 classes.\n",
      "Found 5980 images belonging to 500 classes.\n",
      "Number of classes: 500\n",
      "Number of training samples: 14835\n",
      "Number of validation samples: 5980\n",
      "Class indices saved to C:\\College_programs\\MVT_CNN\\qmul_class_indices.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define path to your QMUL-SurvFace subset dataset\n",
    "qmul_subset_path = r\"C:\\College_programs\\MVT_CNN\\qmul_subset\"  # Replace with your actual path\n",
    "\n",
    "# Step 1: Create ImageDataGenerator for preprocessing\n",
    "# Create ImageDataGenerator for QMUL-SurvFace\n",
    "qmul_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n",
    "\n",
    "# Load training data from QMUL-SurvFace\n",
    "qmul_train_generator = qmul_datagen.flow_from_directory(\n",
    "    qmul_subset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation data from QMUL-SurvFace\n",
    "qmul_validation_generator = qmul_datagen.flow_from_directory(\n",
    "    qmul_subset_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Step 3: Verify the number of classes and samples\n",
    "num_classes = len(qmul_train_generator.class_indices)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Number of training samples: {qmul_train_generator.samples}\")\n",
    "print(f\"Number of validation samples: {qmul_validation_generator.samples}\")\n",
    "\n",
    "# Optionally save class indices to a text file for reference\n",
    "output_file_path = r\"C:\\College_programs\\MVT_CNN\\qmul_class_indices.txt\"  # Replace with your desired output path\n",
    "\n",
    "with open(output_file_path, 'w') as f:\n",
    "    for class_name, class_index in qmul_train_generator.class_indices.items():\n",
    "        f.write(f\"{class_index}: {class_name}\\n\")\n",
    "\n",
    "print(f\"Class indices saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model architecture</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load MobileNetV2 without the top layer (include_top=False)\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze all layers except the last 7\n",
    "for layer in base_model.layers[:-10]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers for face recognition\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global average pooling to reduce dimensions\n",
    "\n",
    "x = Dropout(0.2)(x)  # Adjust dropout rate if needed\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)  # Normalize activations\n",
    "\n",
    "x = Dropout(0.25)(x)  # Adjust dropout rate if needed\n",
    "\n",
    "predictions = Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.01))(x)  # L2 regularization\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.0001)  # Adjusted learning rate for fine-tuning\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set up early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Save model architecture visualization\n",
    "plot_model(model,\n",
    "           to_file='model_architecture.png',  # Path where the image will be saved\n",
    "           show_shapes=True,                   # Show input/output shapes\n",
    "           show_layer_names=True)              # Show layer names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the MobileNetV2 model with Imagenet weights on the LFW subset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.0037 - loss: 8.3740 - val_accuracy: 0.2009 - val_loss: 7.5730\n",
      "Epoch 2/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 2s/step - accuracy: 0.0694 - loss: 7.7422 - val_accuracy: 0.2188 - val_loss: 6.9061\n",
      "Epoch 3/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 2s/step - accuracy: 0.1126 - loss: 7.2594 - val_accuracy: 0.2321 - val_loss: 6.5708\n",
      "Epoch 4/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 2s/step - accuracy: 0.1449 - loss: 6.8657 - val_accuracy: 0.2500 - val_loss: 6.4539\n",
      "Epoch 5/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 3s/step - accuracy: 0.2017 - loss: 6.5754 - val_accuracy: 0.2768 - val_loss: 6.2775\n",
      "Epoch 6/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 3s/step - accuracy: 0.2596 - loss: 6.3027 - val_accuracy: 0.3170 - val_loss: 5.9575\n",
      "Epoch 7/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 2s/step - accuracy: 0.3398 - loss: 6.0723 - val_accuracy: 0.3571 - val_loss: 5.7059\n",
      "Epoch 8/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 3s/step - accuracy: 0.3527 - loss: 5.8220 - val_accuracy: 0.4152 - val_loss: 5.3280\n",
      "Epoch 9/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 3s/step - accuracy: 0.4102 - loss: 5.6458 - val_accuracy: 0.4420 - val_loss: 5.1372\n",
      "Epoch 10/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 3s/step - accuracy: 0.4691 - loss: 5.4089 - val_accuracy: 0.4286 - val_loss: 4.9782\n",
      "Epoch 11/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.5170 - loss: 5.2333 - val_accuracy: 0.4375 - val_loss: 4.8290\n",
      "Epoch 12/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.5828 - loss: 5.0650 - val_accuracy: 0.4464 - val_loss: 4.7247\n",
      "Epoch 13/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.6430 - loss: 4.8981 - val_accuracy: 0.4598 - val_loss: 4.6064\n",
      "Epoch 14/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.6772 - loss: 4.7324 - val_accuracy: 0.4598 - val_loss: 4.5848\n",
      "Epoch 15/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.7457 - loss: 4.5512 - val_accuracy: 0.4688 - val_loss: 4.5128\n",
      "Epoch 16/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.7483 - loss: 4.3624 - val_accuracy: 0.4777 - val_loss: 4.4488\n",
      "Epoch 17/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.7801 - loss: 4.2878 - val_accuracy: 0.4821 - val_loss: 4.3630\n",
      "Epoch 18/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.7941 - loss: 4.2383 - val_accuracy: 0.4732 - val_loss: 4.3177\n",
      "Epoch 19/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.8180 - loss: 4.1318 - val_accuracy: 0.4688 - val_loss: 4.2624\n",
      "Epoch 20/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.8467 - loss: 3.9815 - val_accuracy: 0.4821 - val_loss: 4.2734\n",
      "Epoch 21/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.8561 - loss: 3.8714 - val_accuracy: 0.4688 - val_loss: 4.2284\n",
      "Epoch 22/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.8540 - loss: 3.8594 - val_accuracy: 0.4821 - val_loss: 4.0963\n",
      "Epoch 23/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.8658 - loss: 3.8022 - val_accuracy: 0.4866 - val_loss: 4.0479\n",
      "Epoch 24/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.9293 - loss: 3.5809 - val_accuracy: 0.5134 - val_loss: 4.0381\n",
      "Epoch 25/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 2s/step - accuracy: 0.8968 - loss: 3.5579 - val_accuracy: 0.5089 - val_loss: 4.0023\n",
      "Epoch 26/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 2s/step - accuracy: 0.9137 - loss: 3.4233 - val_accuracy: 0.4821 - val_loss: 4.0377\n",
      "Epoch 27/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 2s/step - accuracy: 0.9327 - loss: 3.2919 - val_accuracy: 0.4911 - val_loss: 4.0917\n",
      "Epoch 28/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2s/step - accuracy: 0.9291 - loss: 3.3221 - val_accuracy: 0.5268 - val_loss: 3.9587\n",
      "Epoch 29/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 2s/step - accuracy: 0.9405 - loss: 3.2462 - val_accuracy: 0.5179 - val_loss: 3.9485\n",
      "Epoch 30/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 2s/step - accuracy: 0.9267 - loss: 3.2294 - val_accuracy: 0.5223 - val_loss: 3.9239\n",
      "Epoch 31/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9599 - loss: 3.0829 - val_accuracy: 0.5134 - val_loss: 3.9046\n",
      "Epoch 32/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.9608 - loss: 2.9838 - val_accuracy: 0.5223 - val_loss: 3.9699\n",
      "Epoch 33/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.9526 - loss: 3.0074 - val_accuracy: 0.5312 - val_loss: 3.9040\n",
      "Epoch 34/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.9531 - loss: 2.9494 - val_accuracy: 0.5312 - val_loss: 3.9491\n",
      "Epoch 35/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - accuracy: 0.9491 - loss: 2.8817 - val_accuracy: 0.5446 - val_loss: 3.9147\n",
      "Epoch 36/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.9613 - loss: 2.8505 - val_accuracy: 0.5491 - val_loss: 3.9334\n",
      "Epoch 37/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9580 - loss: 2.7871 - val_accuracy: 0.5446 - val_loss: 3.9109\n",
      "Epoch 38/40\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.9771 - loss: 2.6963 - val_accuracy: 0.5580 - val_loss: 3.9320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x15294769700>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Set up TensorBoard logging directory\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"lfw_model\")  # Change 'model_name' as needed\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Assuming you have your data generators ready:\n",
    "model.fit(\n",
    "    lfw_train_generator,\n",
    "    validation_data=lfw_validation_generator,\n",
    "    epochs= 40,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the MobileNetV2 model with Imagenet weights on QMUL subset<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\py_envs\\campusgenie\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1947s\u001b[0m 2s/step - accuracy: 0.0723 - loss: 6.6401 - val_accuracy: 0.0766 - val_loss: 6.2234\n",
      "Epoch 2/25\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1753s\u001b[0m 2s/step - accuracy: 0.2614 - loss: 4.8975 - val_accuracy: 0.1388 - val_loss: 5.4723\n",
      "Epoch 3/25\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1792s\u001b[0m 2s/step - accuracy: 0.3640 - loss: 4.0573 - val_accuracy: 0.1828 - val_loss: 4.9999\n",
      "Epoch 4/25\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1848s\u001b[0m 2s/step - accuracy: 0.4442 - loss: 3.5176 - val_accuracy: 0.2502 - val_loss: 4.4221\n",
      "Epoch 5/25\n",
      "\u001b[1m928/928\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1873s\u001b[0m 2s/step - accuracy: 0.5101 - loss: 3.1579 - val_accuracy: 0.3226 - val_loss: 3.9219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1529609e600>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Set up TensorBoard logging directory\n",
    "log_dir = os.path.join(\"logs\", \"fit\", \"qmul_model\")  # Change 'qmul_model' as needed\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Assuming you have your data generators ready:\n",
    "model.fit(\n",
    "    qmul_train_generator,\n",
    "    validation_data=qmul_validation_generator,\n",
    "    epochs=25,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping, tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check the accuracy of the model on the validation data from LFW and QMUL</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step - accuracy: 0.2346 - loss: 5.4356\n",
      "LFW_Validation accuracy: 0.2679, LFW_Validation loss: 5.3242\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m470s\u001b[0m 1s/step - accuracy: 0.0764 - loss: 6.2715\n",
      "QMUL_Validation accuracy: 0.0766, QMUL_Validation loss: 6.2234\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation data from LFW\n",
    "val_loss, val_accuracy = model.evaluate(lfw_validation_generator)\n",
    "print(f'LFW_Validation accuracy: {val_accuracy:.4f}, LFW_Validation loss: {val_loss:.4f}')\n",
    "val_loss, val_accuracy = model.evaluate(qmul_validation_generator)\n",
    "print(f'QMUL_Validation accuracy: {val_accuracy:.4f}, QMUL_Validation loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save the model in Keras 3 format</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in Keras 3 format\n",
    "model.save(r\"C:\\College_programs\\MVT_CNN\\lfw_mnv2.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Detect Faces in a Video using pretrained YOLOv5Face model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from yolo5face.get_model import get_model\n",
    "\n",
    "# Load your YOLOv5Face model\n",
    "model = get_model(\"yolov5n\", device=\"cpu\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = r\"C:\\College_programs\\MVT_CNN\\Adam Sandler Funniest Moments.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize frame for faster processing (optional)\n",
    "    frame = cv2.resize(frame, (224, 224))\n",
    "\n",
    "    # Convert frame to RGB as the model expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform inference with the specified target size\n",
    "    boxes, key_points, scores = model(rgb_frame, target_size=224)\n",
    "\n",
    "    # Process results\n",
    "    for i in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[i]  # Get bounding box coordinates\n",
    "        conf = scores[i]  # Get confidence score\n",
    "\n",
    "        if conf > 0.5:  # Confidence threshold\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)  # Draw rectangle around detected face\n",
    "\n",
    "            # Optionally draw key points if needed (e.g., facial landmarks)\n",
    "            for point in key_points[i]:\n",
    "                cv2.circle(frame, (int(point[0]), int(point[1])), 3, (0, 255, 0), -1)  # Draw key points\n",
    "\n",
    "    # Display the resulting frame with detected faces\n",
    "    cv2.imshow('Video Face Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('v'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Detect Faces in a Video using pretrained YOLOv5Face model and identify with my custom trained model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from yolo5face.get_model import get_model\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load your YOLOv5Face model\n",
    "model = get_model(\"yolov5n\", device=\"cpu\")\n",
    "\n",
    "# Load your identification model (LFW model)\n",
    "identification_model = load_model(r\"C:\\College_programs\\MVT_CNN\\lfw_mnv2.keras\")\n",
    "\n",
    "# Load class indices from the text file\n",
    "class_indices_path = r\"C:\\College_programs\\MVT_CNN\\lfw_class_indices.txt\"\n",
    "class_names = {}\n",
    "\n",
    "with open(class_indices_path, 'r') as f:\n",
    "    for line in f:\n",
    "        index, name = line.strip().split(': ')\n",
    "        class_names[int(index)] = name  # Store in dictionary\n",
    "\n",
    "# Open the video file\n",
    "video_path = r\"C:\\College_programs\\MVT_CNN\\Adam Sandler Funniest Moments.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get original dimensions for scaling later\n",
    "    original_height, original_width = frame.shape[:2]\n",
    "\n",
    "    # Resize frame for faster processing\n",
    "    frame_resized = cv2.resize(frame, (224, 224))\n",
    "\n",
    "    # Convert frame to RGB as the model expects RGB input\n",
    "    rgb_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform inference with the specified target size\n",
    "    boxes, key_points, scores = model(rgb_frame, target_size=224)\n",
    "\n",
    "    # Prepare a list to hold detected faces for batch processing\n",
    "    detected_faces = []\n",
    "    face_boxes = []  # To store original bounding box coordinates\n",
    "    \n",
    "    # Process results\n",
    "    for i in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[i]  # Get bounding box coordinates in resized dimensions\n",
    "        conf = scores[i]  # Get confidence score\n",
    "\n",
    "        if conf > 0.5:  # Confidence threshold\n",
    "            # Scale back coordinates to original frame size\n",
    "            x1 = int(x1 * (original_width / 224))\n",
    "            y1 = int(y1 * (original_height / 224))\n",
    "            x2 = int(x2 * (original_width / 224))\n",
    "            y2 = int(y2 * (original_height / 224))\n",
    "\n",
    "            # Draw rectangle around detected face on the original frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "            # Crop and resize the detected face for identification model\n",
    "            cropped_face = frame[y1:y2, x1:x2]\n",
    "            cropped_face_resized = cv2.resize(cropped_face, (224, 224))  # Resize for identification\n",
    "            \n",
    "            # Append the preprocessed face and its box coordinates to lists\n",
    "            detected_faces.append(cropped_face_resized)\n",
    "            face_boxes.append((x1, y1))  # Store top-left corner for labeling\n",
    "\n",
    "    # If there are detected faces, run identification on the batch\n",
    "    if detected_faces:\n",
    "        # Convert list to numpy array and add batch dimension\n",
    "        faces_array = np.array(detected_faces)\n",
    "        \n",
    "        # Run inference on the identification model\n",
    "        predictions = identification_model.predict(faces_array)\n",
    "\n",
    "        # Process predictions and overlay them on the video frame\n",
    "        for i, prediction in enumerate(predictions):\n",
    "            predicted_class_index = np.argmax(prediction)  # Get index of highest probability class\n",
    "            confidence_score = np.max(prediction)  # Get confidence score for that class\n",
    "            \n",
    "            # Use class_names mapping to get the corresponding name\n",
    "            name = class_names.get(predicted_class_index, \"Unknown\")  # Default to \"Unknown\" if not found\n",
    "            \n",
    "            label = f\"{name}, Conf: {confidence_score:.2f}\"  # Format label\n",
    "            \n",
    "            # Get coordinates for placing text\n",
    "            x, y = face_boxes[i]\n",
    "            \n",
    "            # Display label near the detected face\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    # Display the resulting frame with detected faces and labels\n",
    "    cv2.imshow('Video Face Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('v'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "campusgenie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
